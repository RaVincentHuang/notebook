## 早期尝试

**LeNet**

## 突破进展

**AlexNet**
	Dropout
	Relu
	分组

## 加深

**VGGNet**
## 分组卷积
**Inception**
## ResNet
_梯度消失_
## 大核
**ConvNet**, **RepLKNet**

## 轻量化

目标
+ 效果好精度高
+ 可学习，好训练
+ 精简——参数计算量小
+ 好迁移（VGG > moblieNet）

方向（深/宽）【感受野放大】
+ 参数多——过拟合
+ 梯度爆炸，梯度消失

*NAS（Network Arch Search）*

方法
+ 结构化方法：分组卷积，残差结构
+ 创新提取方式：特殊卷积，提取操作，训练方法
+ 正则化、注意力机制

## 普通卷积

## Group Convolution
沿通道拆 -->通道内部做卷积

## Depthwise Convolution
g = k = C的分组卷积
仅提取空间信息

## Pointwise Convolution
$1 \times 1 \times k$的卷积
仅提取通道信息

**Network in Network**
**SqueezeNet**

多通道使得特征更稀疏

## 通道分离卷积
激活函数会加快内层的收敛
降维不做ReLU
Relu会带来信息丢失，维度越低ReLU丢的信息越多

dying relu problem：某些relu节点始终为0。大负数偏置导致的。

**shufflenet**
解耦合：避免路径重复导致的过拟合

## 全卷积结构
FN跟图像空间结构相关。
拉平尺度受限。
卷积不破坏空间结构。
使用global average pooling替代拉平

## 小卷积堆叠

有效感受野随堆叠线性增长
解耦合
金字塔结构

卷积层堆叠-->梯度消失

## ResNet

like*集成学习*
	dropout

