## [[Stochastic Gradient Descent (SGD)]]
+ 优点：迭代速度快，适合大数据集
+ 缺点：
	+ 抽样梯度存在误差，需要更多的迭代去收敛，后期需要减小学习率
	+ 引起学习曲线震荡
### 学习率
线性衰减
加窗操作（平滑操作）累积窗口 *RMSProp*
动量
	参数$\alpha$是之前梯度的影响
**鞍点**
参数自适应
	减少快速变化的参数的学习率，反之增加 *AdaGrad*

**Adam**

热启动
放大loss


### Batch size
